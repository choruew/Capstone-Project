import os
import json
import time
import torch
from tqdm import tqdm
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from langchain.chains import RetrievalQA
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from sentence_transformers import SentenceTransformer, util
from peft import PeftModel  # æ–°å¢è¼‰å…¥ PEFT æ¨¡çµ„


# è¨­å®šæª”æ¡ˆè·¯å¾‘
pdf_root_folder = r"C:\Users\ai\Desktop\æ˜¥æ—¥éƒ¨\RAGæº–ç¢ºåº¦æ¸¬è©¦\å·²ç”Ÿæˆå•é¡Œçš„è³‡æ–™"
faiss_db_path = r"C:\Users\ai\Desktop\Taiwan Llama faiss"
json_file_path = r"C:\Users\ai\Desktop\æ˜¥æ—¥éƒ¨\RAGæº–ç¢ºåº¦æ¸¬è©¦\é©—è­‰é›†.json"

# è¨­å®šæ¨¡å‹
base_model_name = "yentinglin/Llama-3-Taiwan-8B-Instruct"
# è«‹å°‡æ­¤è·¯å¾‘æ›æˆæ‚¨å¾®èª¿å¾Œçš„æ¬Šé‡è³‡æ–™å¤¾è·¯å¾‘
finetuned_weights_path = r"C:\Users\ai\Desktop\æ˜¥æ—¥éƒ¨\fine tune\å¾®èª¿å¾Œçš„æ¨¡å‹\Taiwan-Llama-3\final_model"
similarity_model_name = "GanymedeNil/text2vec-large-chinese"

def clear_vector_database():
    """æ¸…ç† FAISS å‘é‡è³‡æ–™åº«"""
    if os.path.exists(faiss_db_path):
        for file_name in os.listdir(faiss_db_path):
            file_path = os.path.join(faiss_db_path, file_name)
            if os.path.isfile(file_path):
                os.remove(file_path)
        print("âœ… å‘é‡è³‡æ–™åº«å·²æ¸…ç†å®Œç•¢ã€‚")

def extract_text_from_pdfs(pdf_root_folder):
    """å¾ PDF ä¸­æå–æ–‡æœ¬ï¼Œä¸¦é¡¯ç¤ºé€²åº¦æ¢"""
    documents = []
    pdf_files = [os.path.join(root, file)
                 for root, _, files in os.walk(pdf_root_folder)
                 for file in files if file.lower().endswith(".pdf")]

    print(f"ğŸ“‚ æ‰¾åˆ° {len(pdf_files)} å€‹ PDFï¼Œé–‹å§‹è®€å–...")
    progress_bar = tqdm(total=len(pdf_files), desc="è®€å– PDF", unit="file")

    for pdf_path in pdf_files:
        try:
            loader = PyMuPDFLoader(pdf_path)
            documents.extend(loader.load())
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼šç„¡æ³•è®€å– {pdf_path}ï¼ŒåŸå› ï¼š{e}")
        progress_bar.update(1)

    progress_bar.close()
    print(f"âœ… è®€å–å®Œæˆï¼Œå…±æå– {len(documents)} æ®µæ–‡æœ¬ã€‚")
    return documents

def split_text(documents):
    """æ–‡æœ¬åˆ‡å‰²ï¼ˆåŠ ä¸Šé€²åº¦æ¢ï¼‰"""
    print("âœ‚ï¸  æ­£åœ¨åˆ‡å‰²æ–‡æœ¬...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=400)

    progress_bar = tqdm(total=len(documents), desc="æ–‡æœ¬åˆ‡å‰²", unit="æ®µ")
    all_splits = []
    
    for doc in documents:
        all_splits.extend(text_splitter.split_documents([doc]))
        progress_bar.update(1)
    
    progress_bar.close()
    print(f"âœ… æ–‡æœ¬åˆ‡å‰²å®Œæˆï¼Œå…±ç”¢ç”Ÿ {len(all_splits)} å€‹å€å¡Šã€‚")
    return all_splits

def create_vector_database(documents):
    """å‰µå»º FAISS å‘é‡è³‡æ–™åº«ï¼ˆåŠ ä¸Šé€²åº¦æ¢ï¼‰"""
    print("ğŸ” æ­£åœ¨å‰µå»ºæ–°çš„å‘é‡è³‡æ–™åº«...")

    embedding = HuggingFaceEmbeddings(
        model_name=similarity_model_name,
        encode_kwargs={"batch_size": 32},  # æ‰¹é‡è™•ç†
        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
    )

    # åˆ†æ‰¹è™•ç†åµŒå…¥ï¼Œä¸¦æ›´æ–°é€²åº¦æ¢
    progress_bar = tqdm(total=len(documents), desc="è™•ç†å‘é‡", unit="chunk")
    all_chunks = []
    for i in range(0, len(documents), 32):  # 32 ç‚º batch size
        batch_docs = documents[i:i+32]
        _ = embedding.embed_documents([doc.page_content for doc in batch_docs])
        all_chunks.extend(batch_docs)
        progress_bar.update(len(batch_docs))
    progress_bar.close()

    # å»ºç«‹ FAISS å‘é‡è³‡æ–™åº«
    print("ğŸ” å»ºç«‹ FAISS è³‡æ–™åº«ä¸­...")
    vectordb = FAISS.from_documents(all_chunks, embedding)
    vectordb.save_local(faiss_db_path)
    print(f"âœ… å‘é‡è³‡æ–™åº«å„²å­˜å®Œæˆ: {faiss_db_path}")
    return vectordb

def setup_qa_system(vectordb):
    """è¨­ç½® LLM å•ç­”ç³»çµ±ï¼Œä½¿ç”¨å¾®èª¿å¾Œä¸¦æ•´åˆ RAG çš„æ¨¡å‹"""
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    model = AutoModelForCausalLM.from_pretrained(
        base_model_name, 
        device_map="auto",  
        offload_folder="offload",  
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    )
    # è¼‰å…¥å¾®èª¿å¾Œçš„æ¬Šé‡
    model = PeftModel.from_pretrained(model, finetuned_weights_path)

    tokenizer.pad_token_id = tokenizer.eos_token_id  
    text_generation_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=30,  
        do_sample=True,
        temperature=0.6,
        top_p=0.9
    )

    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)
    retriever = vectordb.as_retriever(search_kwargs={"k": 1})
    return RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)

def load_questions(json_file_path):
    """è®€å– JSON å•é¡Œé›†"""
    with open(json_file_path, 'r', encoding='utf-8') as file:
        return json.load(file)

def evaluate_model(qa_chain, json_file_path):
    """ä½¿ç”¨ JSON å•é¡Œé›†æ¸¬è©¦å•ç­”ç³»çµ±æº–ç¢ºåº¦ï¼Œç”Ÿæˆå¾Œç«‹å³å°å‡ºå•é¡Œèˆ‡ç”Ÿæˆç­”æ¡ˆ"""
    questions = load_questions(json_file_path)
    sentence_model = SentenceTransformer(similarity_model_name, device="cuda" if torch.cuda.is_available() else "cpu")

    questions_list = [entry["question"] for entry in questions]
    reference_answers = [entry["answer"] for entry in questions]

    generated_answers = []
    for q in questions_list:
        prompt = f"è«‹ä»¥ç¹é«”ä¸­æ–‡å›ç­”ä»¥ä¸‹å•é¡Œï¼š\n{q}"
        result = qa_chain.invoke({"query": prompt})["result"].strip()
        print("å•é¡Œï¼š", q)
        print("ç”Ÿæˆç­”æ¡ˆï¼š", result)
        print("-" * 50)
        generated_answers.append(result)

    # è¨ˆç®—èªæ„ç›¸ä¼¼åº¦
    reference_embeddings = sentence_model.encode(reference_answers, batch_size=16, convert_to_tensor=True)
    generated_embeddings = sentence_model.encode(generated_answers, batch_size=16, convert_to_tensor=True)
    similarities = util.pytorch_cos_sim(reference_embeddings, generated_embeddings).diagonal().tolist()
    avg_similarity = sum(similarities) / len(similarities)
    print(f"âœ… å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}")

    del reference_embeddings, generated_embeddings
    torch.cuda.empty_cache()



def main():
    """ä¸»ç¨‹åºåŸ·è¡Œæµç¨‹"""
    start_time = time.time()
    clear_vector_database()
    
    documents = extract_text_from_pdfs(pdf_root_folder)
    if not documents:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• PDF æ–‡ä»¶ï¼Œç¨‹åºçµæŸã€‚")
        return
    
    text_chunks = split_text(documents)
    vectordb = create_vector_database(text_chunks)
    qa_chain = setup_qa_system(vectordb)
    
    end_time = time.time()
    print(f"â³ ç¸½åŸ·è¡Œæ™‚é–“: {end_time - start_time:.2f} ç§’")
    
    evaluate_model(qa_chain, json_file_path)

if __name__ == "__main__":
    main()
