import os
import json
import time
import torch
from tqdm import tqdm
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from langchain.chains import RetrievalQA
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from sentence_transformers import SentenceTransformer, util
from peft import PeftModel

# === å›ºå®šä½¿ç”¨ GPU0ï¼Œè‹¥ç„¡ GPU å‰‡ä½¿ç”¨ CPU ===
if torch.cuda.is_available():
    device = "cuda:0"
else:
    device = "cpu"

print("PyTorch åµæ¸¬åˆ°çš„è£ç½®ï¼š", device)
print("torch.cuda.is_available() =", torch.cuda.is_available())
print("torch.cuda.device_count() =", torch.cuda.device_count())
if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        print(f"è£ç½® {i}:", torch.cuda.get_device_name(i))

# === æ¸¬è©¦ GPU é‹ç®—ç”¨å‡½å¼ ===
def test_gpu_usage():
    if device.startswith("cuda"):
        print("\n[GPUæ¸¬è©¦] æ­£åœ¨é€²è¡Œå¤§çŸ©é™£é‹ç®—...")
        start_time = time.time()
        a = torch.randn((10000, 10000), device=device)
        b = torch.randn((10000, 10000), device=device)
        c = torch.matmul(a, b)
        torch.cuda.synchronize()  # ç­‰å¾…é‹ç®—å®Œæˆ
        end_time = time.time()
        print("[GPUæ¸¬è©¦] çŸ©é™£ç›¸ä¹˜å®Œæˆï¼Œè€—æ™‚:", f"{end_time - start_time:.2f} ç§’")
        print("c.shape =", c.shape)
    else:
        print("\n[GPUæ¸¬è©¦] ç•¶å‰ç‚º CPU æ¨¡å¼ï¼Œç„¡æ³•é€²è¡Œ GPU æ¸¬è©¦ã€‚")

# === è¨­å®šæª”æ¡ˆè·¯å¾‘ ===
pdf_root_folder = r"C:\Users\ai\Desktop\æ˜¥æ—¥éƒ¨\RAGæº–ç¢ºåº¦æ¸¬è©¦\å·²ç”Ÿæˆå•é¡Œçš„è³‡æ–™"
faiss_db_path = r"C:\Users\ai\Desktop\faiss"
json_file_path = r"C:\Users\ai\Desktop\æ˜¥æ—¥éƒ¨\RAGæº–ç¢ºåº¦æ¸¬è©¦\é©—è­‰é›†.json"

# === æ¨¡å‹åç¨± ===
model_name = "mistralai/Mistral-7B-Instruct-v0.2"
similarity_model_name = "GanymedeNil/text2vec-large-chinese"

def clear_vector_database():
    """æ¸…ç† FAISS å‘é‡è³‡æ–™åº«"""
    if os.path.exists(faiss_db_path):
        for file_name in os.listdir(faiss_db_path):
            file_path = os.path.join(faiss_db_path, file_name)
            if os.path.isfile(file_path):
                os.remove(file_path)
        print("âœ… å‘é‡è³‡æ–™åº«å·²æ¸…ç†å®Œç•¢ã€‚")

def extract_text_from_pdfs(pdf_root_folder):
    """å¾ PDF ä¸­æå–æ–‡æœ¬ï¼Œä¸¦é¡¯ç¤ºé€²åº¦æ¢"""
    documents = []
    pdf_files = [
        os.path.join(root, file)
        for root, _, files in os.walk(pdf_root_folder)
        for file in files if file.lower().endswith(".pdf")
    ]

    print(f"ğŸ“‚ æ‰¾åˆ° {len(pdf_files)} å€‹ PDFï¼Œé–‹å§‹è®€å–...")
    progress_bar = tqdm(total=len(pdf_files), desc="è®€å– PDF", unit="file")

    for pdf_path in pdf_files:
        try:
            loader = PyMuPDFLoader(pdf_path)
            documents.extend(loader.load())
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼šç„¡æ³•è®€å– {pdf_path}ï¼ŒåŸå› ï¼š{e}")
        progress_bar.update(1)

    progress_bar.close()
    print(f"âœ… è®€å–å®Œæˆï¼Œå…±æå– {len(documents)} æ®µæ–‡æœ¬ã€‚")
    return documents

def split_text(documents):
    """æ–‡æœ¬åˆ‡å‰²ï¼ˆåŠ ä¸Šé€²åº¦æ¢ï¼‰"""
    print("âœ‚ï¸  æ­£åœ¨åˆ‡å‰²æ–‡æœ¬...")  # å°å‡ºæç¤ºè¨Šæ¯è¡¨ç¤ºé–‹å§‹åˆ‡å‰²æ–‡æœ¬
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=400)  # ä½¿ç”¨éè¿´å¼å­—å…ƒåˆ‡å‰²å™¨ï¼Œè¨­å®šæ¯æ®µæœ€å¤§800å­—å…ƒï¼Œé‡ç–Š400å­—å…ƒ
    progress_bar = tqdm(total=len(documents), desc="æ–‡æœ¬åˆ‡å‰²", unit="æ®µ")  # å»ºç«‹é€²åº¦æ¢ï¼Œç¸½æ•¸ç‚ºæ–‡ä»¶æ•¸é‡ï¼Œå–®ä½ç‚ºã€Œæ®µã€
    all_splits = []  # å„²å­˜æ‰€æœ‰åˆ‡å‰²å¾Œçš„å€å¡Š
    for doc in documents:  # é€ä¸€è™•ç†æ¯å€‹æ–‡ä»¶
        all_splits.extend(text_splitter.split_documents([doc]))  # å°‡åˆ‡å‰²çµæœåŠ å…¥ all_splits
        progress_bar.update(1)  # æ›´æ–°é€²åº¦æ¢
    progress_bar.close()  # é—œé–‰é€²åº¦æ¢
    print(f"âœ… æ–‡æœ¬åˆ‡å‰²å®Œæˆï¼Œå…±ç”¢ç”Ÿ {len(all_splits)} å€‹å€å¡Šã€‚")  # å°å‡ºå®Œæˆè¨Šæ¯èˆ‡å€å¡Šç¸½æ•¸
    return all_splits  # å›å‚³åˆ‡å‰²å¾Œçš„æ‰€æœ‰å€å¡Š

def create_vector_database(documents):
    """å‰µå»º FAISS å‘é‡è³‡æ–™åº«ï¼ˆåŠ ä¸Šé€²åº¦æ¢ï¼‰"""
    print("ğŸ” æ­£åœ¨å‰µå»ºæ–°çš„å‘é‡è³‡æ–™åº«...")  # å°å‡ºæç¤ºè¨Šæ¯
    embedding = HuggingFaceEmbeddings(
        model_name=similarity_model_name,  # æŒ‡å®šè¦ä½¿ç”¨çš„èªæ„åµŒå…¥æ¨¡å‹
        encode_kwargs={"batch_size": 32},  # è¨­å®šæ‰¹æ¬¡å¤§å°ç‚º 32
        model_kwargs={'device': device}  # è¨­å®šæ¨¡å‹åŸ·è¡Œçš„è£ç½®ï¼ˆå¦‚ CPU æˆ– GPUï¼‰
    )

    progress_bar = tqdm(total=len(documents), desc="è™•ç†å‘é‡", unit="chunk")  # å»ºç«‹é€²åº¦æ¢ï¼Œç”¨æ–¼è¿½è¹¤å‘é‡è™•ç†é€²åº¦
    all_chunks = []  # å„²å­˜æ‰€æœ‰å‘é‡åŒ–è™•ç†éçš„å€å¡Š
    for i in range(0, len(documents), 32):  # æ¯ 32 ç­†æ–‡ä»¶ç‚ºä¸€æ‰¹é€²è¡Œè™•ç†
        batch_docs = documents[i:i+32]  # å–å¾—ç•¶å‰æ‰¹æ¬¡çš„æ–‡ä»¶
        _ = embedding.embed_documents([doc.page_content for doc in batch_docs])  # å°æ¯å€‹æ–‡ä»¶å…§å®¹é€²è¡ŒåµŒå…¥ï¼ˆè½‰æˆå‘é‡ï¼‰
        all_chunks.extend(batch_docs)  # å°‡æœ¬æ‰¹æ¬¡çš„æ–‡ä»¶åŠ å…¥ all_chunks
        progress_bar.update(len(batch_docs))  # ä¾ç…§è™•ç†æ•¸é‡æ›´æ–°é€²åº¦æ¢
    progress_bar.close()  # é—œé–‰é€²åº¦æ¢

    print("ğŸ” å»ºç«‹ FAISS è³‡æ–™åº«ä¸­...")  # å°å‡ºå»ºç«‹è³‡æ–™åº«æç¤º
    vectordb = FAISS.from_documents(all_chunks, embedding)  # ä½¿ç”¨è™•ç†å¾Œçš„æ–‡ä»¶èˆ‡åµŒå…¥æ¨¡å‹å»ºç«‹ FAISS å‘é‡è³‡æ–™åº«
    vectordb.save_local(faiss_db_path)  # å°‡è³‡æ–™åº«å„²å­˜åˆ°æœ¬æ©ŸæŒ‡å®šè·¯å¾‘
    print(f"âœ… å‘é‡è³‡æ–™åº«å„²å­˜å®Œæˆ: {faiss_db_path}")  # å°å‡ºå„²å­˜æˆåŠŸè¨Šæ¯
    return vectordb  # å›å‚³å»ºç«‹å¥½çš„ FAISS å‘é‡è³‡æ–™åº«ç‰©ä»¶


def setup_qa_system(vectordb):
    """è¨­ç½® LLM å•ç­”ç³»çµ±ï¼ˆç¢ºä¿åœ¨ GPU ä¸Šé‹è¡Œï¼‰"""
    print("ğŸš€ è¼‰å…¥åŸºåº•æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map={"": device},
        torch_dtype=torch.float16 if device.startswith("cuda") else torch.float32
    )
    model.to(device)

    tokenizer.pad_token_id = tokenizer.eos_token_id
    # æ³¨æ„ï¼šå·²é€é accelerate è¼‰å…¥æ¨¡å‹ï¼Œä¸è¦å†æŒ‡å®š device
    text_generation_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=30,
        temperature=0.6,
        top_p=0.9
    )
    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)
    retriever = vectordb.as_retriever(search_kwargs={"k": 1})
    return RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)

def load_questions(json_file_path):
    """è®€å– JSON å•é¡Œé›†"""
    with open(json_file_path, 'r', encoding='utf-8') as file:
        return json.load(file)

def evaluate_model(qa_chain, json_file_path):
    """ä½¿ç”¨ JSON å•é¡Œé›†æ¸¬è©¦å•ç­”ç³»çµ±æº–ç¢ºåº¦"""
    questions = load_questions(json_file_path)
    sentence_model = SentenceTransformer(similarity_model_name, device=device)

    questions_list = [entry["question"] for entry in questions]
    reference_answers = [entry["answer"] for entry in questions]
    prompts = [f"è«‹ä»¥ç¹é«”ä¸­æ–‡å›ç­”ä»¥ä¸‹å•é¡Œï¼š\n{q}" for q in questions_list]

    batch_size = 5
    generated_answers = []
    for i in range(0, len(prompts), batch_size):
        batch_prompts = prompts[i:i+batch_size]
        batch_answers = [qa_chain.invoke({"query": p})["result"].strip() for p in batch_prompts]
        generated_answers.extend(batch_answers)

    reference_embeddings = sentence_model.encode(reference_answers, batch_size=16, convert_to_tensor=True)
    generated_embeddings = sentence_model.encode(generated_answers, batch_size=16, convert_to_tensor=True)
    similarities = util.pytorch_cos_sim(reference_embeddings, generated_embeddings).diagonal().tolist()
    avg_similarity = sum(similarities) / len(similarities)

    del reference_embeddings, generated_embeddings
    torch.cuda.empty_cache()

    print(f"âœ… å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}")

def main():
    """ä¸»ç¨‹åºåŸ·è¡Œæµç¨‹"""
    # 1. æ¸¬è©¦ GPU é‹ç®—
    test_gpu_usage()

    # 2. é€²è¡Œ RAG æµç¨‹
    start_time = time.time()
    clear_vector_database()
    
    documents = extract_text_from_pdfs(pdf_root_folder)
    if not documents:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• PDF æ–‡ä»¶ï¼Œç¨‹åºçµæŸã€‚")
        return
    
    text_chunks = split_text(documents)
    vectordb = create_vector_database(text_chunks)
    qa_chain = setup_qa_system(vectordb)
    
    end_time = time.time()
    print(f"â³ ç¸½åŸ·è¡Œæ™‚é–“: {end_time - start_time:.2f} ç§’")
    
    evaluate_model(qa_chain, json_file_path)

if __name__ == "__main__":
    main()
