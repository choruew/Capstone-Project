import time
import torch
import json
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from sentence_transformers import SentenceTransformer, util
from langchain_huggingface import HuggingFacePipeline

# === å›ºå®šä½¿ç”¨ GPU0ï¼Œè‹¥ç„¡ GPU å‰‡ä½¿ç”¨ CPU ===
device = "cuda:0" if torch.cuda.is_available() else "cpu"
print("PyTorch åµæ¸¬åˆ°çš„è£ç½®ï¼š", device)

# === æ¨¡å‹åç¨± ===
model_name = "yentinglin/Llama-3-Taiwan-8B-Instruct"
similarity_model_name = "GanymedeNil/text2vec-large-chinese"

# === å•é¡Œé›†è·¯å¾‘ ===
json_file_path = r"C:\Users\ai\Desktop\æ˜¥æ—¥éƒ¨\RAGæº–ç¢ºåº¦æ¸¬è©¦\é©—è­‰é›†.json"

def setup_llm_pipeline():
    """è¼‰å…¥ LLM æ¨¡å‹èˆ‡ç”Ÿæˆç®¡ç·š"""
    print("ğŸš€ è¼‰å…¥åŸºåº•æ¨¡å‹ä¸­...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map={"": device},
        torch_dtype=torch.float16 if device.startswith("cuda") else torch.float32
    )
    model.to(device)
    tokenizer.pad_token_id = tokenizer.eos_token_id

    text_generation_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=30,
        temperature=0.6,
        top_p=0.9,
        do_sample=True  # å•Ÿç”¨æŠ½æ¨£æ‰æœƒä½¿ç”¨ temperature / top_p
    )
    return HuggingFacePipeline(pipeline=text_generation_pipeline)

def load_questions(json_file_path):
    """è®€å– JSON å•é¡Œé›†"""
    with open(json_file_path, 'r', encoding='utf-8') as file:
        return json.load(file)

def evaluate_model(llm, json_file_path):
    """ä½¿ç”¨ JSON å•é¡Œé›†æ¸¬è©¦å•ç­”ç³»çµ±æº–ç¢ºåº¦"""
    questions = load_questions(json_file_path)
    sentence_model = SentenceTransformer(similarity_model_name, device=device)

    questions_list = [entry["question"] for entry in questions]
    reference_answers = [entry["answer"] for entry in questions]
    prompts = [f"è«‹ä»¥ç¹é«”ä¸­æ–‡å›ç­”ä»¥ä¸‹å•é¡Œï¼š\n{q}" for q in questions_list]

    batch_size = 5
    generated_answers = []
    for i in range(0, len(prompts), batch_size):
        batch_prompts = prompts[i:i+batch_size]
        batch_outputs = [llm.invoke(p).strip() for p in batch_prompts]  # ä¿®æ­£é»ï¼šå›å‚³å€¼ç‚ºå­—ä¸²
        generated_answers.extend(batch_outputs)

    reference_embeddings = sentence_model.encode(reference_answers, batch_size=16, convert_to_tensor=True)
    generated_embeddings = sentence_model.encode(generated_answers, batch_size=16, convert_to_tensor=True)
    similarities = util.pytorch_cos_sim(reference_embeddings, generated_embeddings).diagonal().tolist()
    avg_similarity = sum(similarities) / len(similarities)

    print(f"âœ… å¹³å‡èªæ„ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
    return avg_similarity

def main():
    start_time = time.time()
    llm = setup_llm_pipeline()
    evaluate_model(llm, json_file_path)
    end_time = time.time()
    print(f"â³ åŸ·è¡Œç¸½æ™‚é–“: {end_time - start_time:.2f} ç§’")

if __name__ == "__main__":
    main()
